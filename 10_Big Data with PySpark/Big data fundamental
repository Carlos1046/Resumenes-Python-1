

#Se facilita el entendimiento de Pyspark con:
- map(funcion, lista)
- filter(funcion, lista)
- lambda x: expresion


#RDD - Resilent Distributed Dataset

#Cargar los datos en pyspark usando SparkContext y creación de RDD

sc.parallelize(lista, minPartitions = n) #paralelizar una lista

sc.textFile("archivo.txt") #cargar un archivo de texto

#Transformaciones

map(lambda x : x **2)
filter(lambda x: "texto" in x) ó filter(lambda x:x > 2)
flatMap()
union()


#Acciones, operaciones que se aplican en los RDD para devolver un valor después de ejecutar un cálculo.

collect()
count()
first()
take()

#RDD Pares clave/valor (cada fila tienen una clave y valores)

Ejemplo de RRD regula a RDD par clave/valor

Rdd_r = ["Carlos 28", "Tere" 29]
Rdd_r = sc.paralleliz(Rdd_r)
Rdd_cv = Rdd_r.map(lambda x: ( x.split(" ")[0], x.split(" ")[1] ))

#transformaciones

reduceByKey() #combina valores con la misma clave 
groupByKey() #agrupa los valores por clave
sortByKey() #ordena por orden de clave
join() #une rdd por clave

#acciones















